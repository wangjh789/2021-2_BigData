{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ppjoin2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFrOFUcNinUP",
        "outputId": "e0973008-1e33-4c46-ff16-f4cf39c20ef0"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/gohdong/2021_autumn/master/BigData/project1/facebook_combined.txt \n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz \n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install findspark\n",
        "!pip install pyspark"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-07 15:23:15--  https://raw.githubusercontent.com/gohdong/2021_autumn/master/BigData/project1/facebook_combined.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 854362 (834K) [text/plain]\n",
            "Saving to: ‘facebook_combined.txt.1’\n",
            "\n",
            "facebook_combined.t 100%[===================>] 834.34K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-10-07 15:23:15 (17.8 MB/s) - ‘facebook_combined.txt.1’ saved [854362/854362]\n",
            "\n",
            "--2021-10-07 15:23:18--  https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 224445805 (214M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.1.2-bin-hadoop2.7.tgz.1’\n",
            "\n",
            "spark-3.1.2-bin-had 100%[===================>] 214.05M   118MB/s    in 1.8s    \n",
            "\n",
            "2021-10-07 15:23:20 (118 MB/s) - ‘spark-3.1.2-bin-hadoop2.7.tgz.1’ saved [224445805/224445805]\n",
            "\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.7/dist-packages (1.4.2)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.1.2)\n",
            "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_bmWnIOisxs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "ad49b640-4b65-4b38-9c66-7ef1c878c5fd"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext\n",
        "\n",
        "\n",
        "sc = SparkContext(\"local\",\"ppj\")"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-aa5317d19da7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"ppj\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
            "\u001b[0;32m/content/spark-3.1.2-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    345\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 347\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    348\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=ppj, master=local) created by __init__ at <ipython-input-2-aa5317d19da7>:10 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmRp2Zc7iuJj"
      },
      "source": [
        "undirect = sc.textFile('facebook_combined.txt').map(lambda x : x.split(\" \"))\\\n",
        ".flatMap(lambda x: ((int(x[0]),int(x[1])),(int(x[1]),int(x[0])))).groupByKey().mapValues(list)\n",
        "\n",
        "threshold = 0.6"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU3YKoYEkat8"
      },
      "source": [
        "A =undirect.map(lambda x: len(x[1])).collect()\n",
        "B=sorted(range(len(A)),key=lambda x:A[x])\n",
        "order=sorted(range(len(A)),key=lambda x:B[x])\n",
        "#sort를 위한 global order"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH6KJ422tli_"
      },
      "source": [
        "sorted_undirect = undirect.mapValues(lambda x: sorted(x, key=lambda y : order[y]))"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybv1aCPSi4cA"
      },
      "source": [
        "ref = sc.broadcast(dict(sorted_undirect.collect()))"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRH-lE8y2W0N"
      },
      "source": [
        "def make_invert(item):\n",
        "    key = item[0]\n",
        "    arr= item[1]\n",
        "    result= []\n",
        "    prefix_len = int(len(ref.value[key]) - threshold*len(ref.value[key]) + 1) #prefix_filtering \n",
        "    for i in arr[:prefix_len]:\n",
        "      result.append((i,key))\n",
        "    return result"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Odro4uIL2c6L",
        "outputId": "112b11c5-cd9b-496f-96dc-cec0dbfaa9a0"
      },
      "source": [
        "invert = sorted_undirect.flatMap(make_invert).groupByKey().mapValues(list)\n",
        "invert.count()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2534"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGBr8u492-AX"
      },
      "source": [
        "def make_pair(item):\n",
        "    arr = item[1]\n",
        "    result = []\n",
        "    for idx,i in enumerate(arr):\n",
        "        for j in arr[idx+1:]:\n",
        "          if not len(ref.value[j]) >= threshold*len(ref.value[i]): continue #size filtering |x| >= t * |y| \n",
        "          result.append(((i,j),1))\n",
        "    return result"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZJzmevf3KGH",
        "outputId": "0ee2cbb2-cdba-4056-ee32-e488f9a7eca7"
      },
      "source": [
        "pairs = invert.flatMap(make_pair).reduceByKey(lambda x,y : x+y) #prefix 부분에서의 overlap 계산\n",
        "#invert != sorted_undirect\n",
        "pairs.count()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "403188"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMUO74ni69Qa"
      },
      "source": [
        "def compute_overlap(item):\n",
        "    x,y = item[0][0],item[0][1]\n",
        "\n",
        "    Px = int(len(ref.value[x]) - threshold*len(ref.value[x]) + 1) #prefix 길이\n",
        "    Py = int(len(ref.value[y]) - threshold*len(ref.value[y]) + 1)\n",
        "\n",
        "    Wx = ref.value[x][Px-1] # prefix의 마지막 토큰\n",
        "    Wy = ref.value[y][Py-1]\n",
        "\n",
        "    share = item[1] # prefix 부분의 overlap 수\n",
        "\n",
        "    a = threshold/(1+threshold)*(len(ref.value[x])+len(ref.value[y]))\n",
        "    value = 0\n",
        "    \n",
        "    if order[Wx] < order[Wy]:                           #x suffix 부분과 y prefix 부분이 겹칠 수 있는 경우\n",
        "      value = share + len(set(ref.value[x][Px:])&set(ref.value[y][share:])) \n",
        "\n",
        "    else :                                              #y suffix 부분과 x prefix 부분이 겹칠 수 있는 경우\n",
        "      value = share + len(set(ref.value[x][share:])&set(ref.value[y][Py:]))\n",
        "\n",
        "    return value >= a\n",
        "\n",
        "def is_not_friend(item):\n",
        "    x = item[0][0]\n",
        "    y = item[0][1]\n",
        "    target,arr = x,ref.value[y]\n",
        "    return not (target in arr)"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmqnsVEE7PNL",
        "outputId": "8ec85b02-4cd5-4730-dc9c-76cc0c105070"
      },
      "source": [
        "result = pairs.filter(compute_overlap).filter(is_not_friend).flatMap(lambda x: ((x[0][0],x[0][1]),(x[0][1],x[0][0])))\n",
        "result.count()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3238"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    }
  ]
}