{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sort_invert.ipynb",
      "provenance": [],
      "mount_file_id": "1tpUNghVXUk8EU_6NM8x0LqzifEezuI7e",
      "authorship_tag": "ABX9TyNOVbSibreEWErOj6waQ4qO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wangjh789/2021-2_BigData/blob/master/sort_invert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z1pNbkwTEGG",
        "outputId": "a97f06f3-037b-4eb8-8fa8-64200d491ffc"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz \n",
        "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
        "!pip install findspark\n",
        "!pip install pyspark\n",
        "!pip install numpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-04 09:40:18--  https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 224445805 (214M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.1.2-bin-hadoop2.7.tgz’\n",
            "\n",
            "spark-3.1.2-bin-had 100%[===================>] 214.05M  82.7MB/s    in 2.6s    \n",
            "\n",
            "2021-11-04 09:40:21 (82.7 MB/s) - ‘spark-3.1.2-bin-hadoop2.7.tgz’ saved [224445805/224445805]\n",
            "\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.7/dist-packages (1.4.2)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K9T2mDwTTht"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkContext\n",
        "\n",
        "\n",
        "sc = SparkContext(\"local[10]\",\"generate_sorted\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xTXujJsTcz8",
        "outputId": "f1ce5f6a-f440-41f3-ad60-c350c8e6a7e9"
      },
      "source": [
        "data1 = []\n",
        "\n",
        "f = open('./drive/MyDrive/inverted_list.txt', 'r')\n",
        "while True:\n",
        "    line = f.readline()\n",
        "    if not line:\n",
        "        break\n",
        "    temp = line.split(' ')\n",
        "\n",
        "    if temp[-1] == '':\n",
        "        data1.append((int(temp[0]),[]))\n",
        "    else:\n",
        "        data1.append((int(temp[0]), [int(i) for i in temp[1:]]))\n",
        "\n",
        "print(len(data1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PupeswLTeiz"
      },
      "source": [
        "invert = sc.parallelize(data1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU3Yg3HFZ_dV"
      },
      "source": [
        "sorted_desc = invert.sortBy(lambda x:len(x[1]),ascending=False)\\\n",
        "              .map(lambda x: ([x[0]]+x[1]))\n",
        "sorted_asc = invert.sortBy(lambda x:len(x[1]))\\\n",
        "              .map(lambda x: ([x[0]]+x[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2j3FEmJXKKs"
      },
      "source": [
        "with open(\"./drive/MyDrive/sorted_asc.txt\", \"w\") as txt_file:\n",
        "    for line in sorted_asc.collect():\n",
        "      for idx,i in enumerate(line):\n",
        "        line[idx] = str(i)\n",
        "      txt_file.write(\" \".join(line) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZdsSYqBdXI8"
      },
      "source": [
        "with open(\"./drive/MyDrive/sorted_desc.txt\", \"w\") as txt_file:\n",
        "    for line in sorted_desc.collect():\n",
        "      for idx,i in enumerate(line):\n",
        "        line[idx] = str(i)\n",
        "      txt_file.write(\" \".join(line) + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}